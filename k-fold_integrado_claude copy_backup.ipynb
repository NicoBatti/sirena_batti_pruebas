{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCCI√ìN: Ejecutar esta celda primero para cargar todas las librer√≠as necesarias\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fijar semillas para reproducibilidad\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"‚úì Librer√≠as importadas correctamente\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTRUCCI√ìN: Ejecutar para cargar el CSV y hacer preprocesamiento b√°sico\n",
    "\n",
    "df = pd.read_csv(\"registros_rio_6746.csv\")\n",
    "\n",
    "# Modificar columnas del df\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values(by=\"date\")\n",
    "\n",
    "\n",
    "# Agregar columnas futuras (las vamos a eliminar despu√©s)\n",
    "df['altura_7_dias'] = df['altura_value'].shift(periods=-7)\n",
    "df['precipitaciones_7_dias'] = df['precipitaciones_value'].shift(periods=-7)\n",
    "df = df.iloc[:1001].copy()  # elimino las ultimas 7 ya que tienen datos del futuro\n",
    "\n",
    "print(f\"‚úì Datos cargados: {len(df)} registros\")\n",
    "print(f\"  Rango de fechas: {df['date'].min()} a {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCCI√ìN: Ejecutar para agregar la columna de estaci√≥n del a√±o con One-Hot Encoding\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def obtener_estacion(fecha):\n",
    "    mes = fecha.month\n",
    "    dia = fecha.day\n",
    "    \n",
    "    if (mes == 12 and dia >= 21) or (mes <= 3 and (mes < 3 or dia <= 20)):\n",
    "        return \"verano\"\n",
    "    elif (mes == 3 and dia >= 21) or (mes <= 6 and (mes < 6 or dia <= 20)):\n",
    "        return \"oto√±o\"\n",
    "    elif (mes == 6 and dia >= 21) or (mes <= 9 and (mes < 9 or dia <= 20)):\n",
    "        return \"invierno\"\n",
    "    else:\n",
    "        return \"primavera\"\n",
    "\n",
    "# Crear la columna base\n",
    "df['estacion'] = df['date'].apply(obtener_estacion)\n",
    "\n",
    "# Aplicar One-Hot Encoding\n",
    "df = pd.get_dummies(df, columns=['estacion'], prefix='est')\n",
    "\n",
    "print(\"‚úì Columnas de estaci√≥n creadas con One-Hot Encoding\\n\")\n",
    "print(\"Nuevas columnas:\")\n",
    "print([col for col in df.columns if col.startswith(\"est_\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ba464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCCI√ìN: Ejecutar para eliminar columnas que no usaremos\n",
    "\n",
    "columnas_a_eliminar = ['Unnamed: 0', 'rio_id', 'lat', 'lon', 'altura_7_dias', 'precipitaciones_7_dias']\n",
    "df = df.drop(columns=columnas_a_eliminar, errors='ignore')\n",
    "\n",
    "print(f\"‚úì Columnas limpiadas\")\n",
    "print(f\"  Columnas finales: {list(df.columns)}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a337c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCCI√ìN: CR√çTICO - Esta celda separa los datos ANTES de hacer K-Fold\n",
    "# El test set NO se tocar√° hasta el final\n",
    "\n",
    "# Primero separamos el TEST SET (20% final del dataset)\n",
    "# Este conjunto NO se usar√° para nada hasta la evaluaci√≥n final\n",
    "test_size = 0.20\n",
    "split_point = int(len(df) * (1 - test_size))\n",
    "\n",
    "df_train_val = df.iloc[:split_point].copy()  # 80% para train+validation\n",
    "df_test = df.iloc[split_point:].copy()       # 20% para test final\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SEPARACI√ìN DE DATOS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Dataset completo: {len(df)} registros\")\n",
    "print(f\"Train+Validation: {len(df_train_val)} registros (80%)\")\n",
    "print(f\"Test (guardado para el final): {len(df_test)} registros (20%)\")\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANTE: El test set NO se usar√° hasta la evaluaci√≥n final\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe458e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCCI√ìN: Ejecutar para definir la funci√≥n de ventanas deslizantes (ajustada a nueva convenci√≥n)\n",
    "import numpy as np\n",
    "\n",
    "# features debe ser: ['altura_value', 'precipitaciones_value', 'est_verano', 'est_oto√±o', 'est_invierno', 'est_primavera']\n",
    "def crear_ventanas(df, features, window_size=3):\n",
    "    \"\"\"\n",
    "    Nuevo comportamiento:\n",
    "    - window_size = n√∫mero de d√≠as pasados a observar (p.ej. 2 -> i-2, i-1).\n",
    "    - la historia incluye adem√°s el d√≠a 'hoy' (i). Por tanto la historia tiene window_size+1 d√≠as: i-window_size .. i.\n",
    "    - se a√±aden como features del \"futuro\" las columnas features[1:] (precip + estaciones) en i+1.\n",
    "    - target = altura_value en i+1.\n",
    "\n",
    "    Esto produce muestras que predicen la altura de ma√±ana (i+1) usando pasado + presente + precip/estaci√≥n de ma√±ana.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    hist_cols = features              # columnas que usamos por d√≠a en la historia (altura, precip, estaciones)\n",
    "    future_cols = features[1:]        # para el d√≠a objetivo solo usamos precip + estaciones\n",
    "\n",
    "    # iteramos t = √≠ndice ¬´hoy¬ª; necesitamos que exista t+1 para target\n",
    "    for t in range(window_size, len(df) - 1):\n",
    "        # historia: desde t-window_size hasta t (inclusive) -> window_size+1 d√≠as\n",
    "        hist_block = df[hist_cols].iloc[t - window_size : t + 1].values.flatten()\n",
    "        # futuro: precip + estaci√≥n en t+1\n",
    "        fut_block = df[future_cols].iloc[t + 1].values.flatten()\n",
    "\n",
    "        features_vector = np.concatenate([hist_block, fut_block])\n",
    "        X.append(features_vector)\n",
    "        # target: altura en t+1\n",
    "        y.append(df['altura_value'].iloc[t + 1])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "print('‚úì crear_ventanas: nueva convenci√≥n aplicada (history: i-window_size..i, futuro: i+1, target: altura i+1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCCI√ìN: Ejecutar para definir el K-Fold temporal (respeta el orden cronol√≥gico)\n",
    "\n",
    "class TimeSeriesKFold:\n",
    "    \"\"\"\n",
    "    K-Fold Cross-Validation para series temporales.\n",
    "    Respeta el orden temporal: cada fold usa datos pasados para entrenar\n",
    "    y datos futuros para validar.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def split(self, X):\n",
    "        n_samples = len(X)\n",
    "        fold_size = n_samples // (self.n_splits + 1)\n",
    "        \n",
    "        for i in range(self.n_splits):\n",
    "            # Train: desde el inicio hasta el punto de corte\n",
    "            train_end = fold_size * (i + 2)\n",
    "            train_indices = np.arange(0, train_end - fold_size)\n",
    "            \n",
    "            # Validation: el fold siguiente\n",
    "            val_start = train_end - fold_size\n",
    "            val_end = train_end\n",
    "            val_indices = np.arange(val_start, val_end)\n",
    "            \n",
    "            yield train_indices, val_indices\n",
    "\n",
    "print(\"‚úì TimeSeriesKFold definido\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ddf11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCCI√ìN: Ejecutar para definir la funci√≥n que crea modelos\n",
    "\n",
    "def crear_modelo(input_shape, arquitectura, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal con la arquitectura especificada.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: N√∫mero de features de entrada\n",
    "        arquitectura: Lista con n√∫mero de neuronas por capa, ej: [64, 32, 16]\n",
    "        learning_rate: Tasa de aprendizaje del optimizador\n",
    "        \n",
    "    Returns:\n",
    "        Modelo compilado\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(input_shape,)))\n",
    "    \n",
    "    # Agregar capas ocultas\n",
    "    for neurons in arquitectura:\n",
    "        model.add(layers.Dense(neurons, activation='relu'))\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    # Compilar\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úì Funci√≥n crear_modelo() definida\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCCI√ìN: Ejecutar para definir qu√© hiperpar√°metros probar\n",
    "# PUEDES MODIFICAR ESTOS VALORES seg√∫n lo que quieras probar\n",
    "\n",
    "# Hiperpar√°metros a probar\n",
    "HYPERPARAMETERS = {\n",
    "    'window_size': [3, 5, 7],  # Tama√±os de ventana\n",
    "    'arquitectura': [\n",
    "        [64, 32, 16],      # Red profunda\n",
    "        [128, 64],         # Red ancha\n",
    "        [32, 32, 32],      # Red uniforme\n",
    "        [64, 32],          # Red simple\n",
    "    ],\n",
    "    'learning_rate': [0.001, 0.0001],  # Tasas de aprendizaje\n",
    "}\n",
    "\n",
    "# Configuraci√≥n del experimento\n",
    "N_SPLITS = 5           # N√∫mero de folds para K-Fold\n",
    "EPOCHS = 100           # √âpocas de entrenamiento\n",
    "BATCH_SIZE = 32        # Tama√±o del batch\n",
    "VERBOSE = 0            # 0=silencioso, 1=verbose\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFIGURACI√ìN DEL EXPERIMENTO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"K-Fold splits: {N_SPLITS}\")\n",
    "print(f\"√âpocas por modelo: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(\"\\nHiperpar√°metros a validar:\")\n",
    "print(f\"  - Window sizes: {HYPERPARAMETERS['window_size']}\")\n",
    "print(f\"  - Arquitecturas: {len(HYPERPARAMETERS['arquitectura'])} variantes\")\n",
    "print(f\"  - Learning rates: {HYPERPARAMETERS['learning_rate']}\")\n",
    "\n",
    "# Calcular total de configuraciones\n",
    "total_configs = (len(HYPERPARAMETERS['window_size']) * \n",
    "                 len(HYPERPARAMETERS['arquitectura']) * \n",
    "                 len(HYPERPARAMETERS['learning_rate']))\n",
    "total_entrenamientos = total_configs * N_SPLITS\n",
    "\n",
    "print(f\"\\nTotal de configuraciones: {total_configs}\")\n",
    "print(f\"Total de entrenamientos: {total_entrenamientos}\")\n",
    "print(f\"‚è±Ô∏è Tiempo estimado: ~{total_entrenamientos * 2 // 60} minutos\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCCI√ìN: ‚ö†Ô∏è ESTA ES LA CELDA PRINCIPAL - Puede tardar varios minutos\n",
    "# Esta celda ejecuta todo el proceso de validaci√≥n cruzada\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INICIANDO K-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "resultados = []\n",
    "config_num = 0\n",
    "\n",
    "# Iterar sobre todas las combinaciones de hiperpar√°metros\n",
    "for window_size, arquitectura, lr in product(\n",
    "    HYPERPARAMETERS['window_size'],\n",
    "    HYPERPARAMETERS['arquitectura'],\n",
    "    HYPERPARAMETERS['learning_rate']\n",
    "):\n",
    "    config_num += 1\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"CONFIGURACI√ìN {config_num}/{total_configs}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"  Window size: {window_size}\")\n",
    "    print(f\"  Arquitectura: {arquitectura}\")\n",
    "    print(f\"  Learning rate: {lr}\")\n",
    "    \n",
    "    # Crear ventanas con este window_size usando SOLO train+val data\n",
    "    X, y = crear_ventanas(df_train_val, features, window_size=window_size)\n",
    "    \n",
    "    # Inicializar K-Fold\n",
    "    kfold = TimeSeriesKFold(n_splits=N_SPLITS)\n",
    "    \n",
    "    # Almacenar scores de cada fold\n",
    "    fold_scores = []\n",
    "    \n",
    "    # Iterar sobre cada fold\n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kfold.split(X), 1):\n",
    "        print(f\"  ‚Üí Fold {fold_num}/{N_SPLITS}... \", end='')\n",
    "        \n",
    "        # Separar train y validation para este fold\n",
    "        X_train_fold = X[train_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        X_val_fold = X[val_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        # Escalar datos (IMPORTANTE: fit solo en train)\n",
    "        scaler_X_fold = MinMaxScaler()\n",
    "        scaler_y_fold = MinMaxScaler()\n",
    "        \n",
    "        X_train_scaled = scaler_X_fold.fit_transform(X_train_fold)\n",
    "        X_val_scaled = scaler_X_fold.transform(X_val_fold)\n",
    "        \n",
    "        y_train_scaled = scaler_y_fold.fit_transform(y_train_fold.reshape(-1, 1)).flatten()\n",
    "        y_val_scaled = scaler_y_fold.transform(y_val_fold.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Crear modelo\n",
    "        tf.random.set_seed(SEED + fold_num)\n",
    "        model = crear_modelo(X_train_scaled.shape[1], arquitectura, lr)\n",
    "        \n",
    "        # Entrenar\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train_scaled,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_val_scaled, y_val_scaled),\n",
    "            verbose=VERBOSE\n",
    "        )\n",
    "        \n",
    "        # Evaluar en validation\n",
    "        y_pred_scaled = model.predict(X_val_scaled, verbose=0)\n",
    "        y_pred = scaler_y_fold.inverse_transform(y_pred_scaled)\n",
    "        y_val_real = scaler_y_fold.inverse_transform(y_val_fold.reshape(-1, 1))\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        mae = mean_absolute_error(y_val_real, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_real, y_pred))\n",
    "        \n",
    "        fold_scores.append({\n",
    "            'mae': mae,\n",
    "            'rmse': rmse\n",
    "        })\n",
    "        \n",
    "        print(f\"MAE: {mae:.4f}m\")\n",
    "    \n",
    "    # Calcular promedios de los folds\n",
    "    mae_mean = np.mean([s['mae'] for s in fold_scores])\n",
    "    mae_std = np.std([s['mae'] for s in fold_scores])\n",
    "    rmse_mean = np.mean([s['rmse'] for s in fold_scores])\n",
    "    rmse_std = np.std([s['rmse'] for s in fold_scores])\n",
    "    \n",
    "    # Guardar resultados de esta configuraci√≥n\n",
    "    resultados.append({\n",
    "        'window_size': window_size,\n",
    "        'arquitectura': str(arquitectura),\n",
    "        'learning_rate': lr,\n",
    "        'mae_mean': mae_mean,\n",
    "        'mae_std': mae_std,\n",
    "        'rmse_mean': rmse_mean,\n",
    "        'rmse_std': rmse_std,\n",
    "        'mae_folds': [s['mae'] for s in fold_scores],\n",
    "        'rmse_folds': [s['rmse'] for s in fold_scores]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n  üìä Resultado promedio ({N_SPLITS} folds):\")\n",
    "    print(f\"     MAE: {mae_mean:.4f} ¬± {mae_std:.4f} m\")\n",
    "    print(f\"     RMSE: {rmse_mean:.4f} ¬± {rmse_std:.4f} m\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì VALIDACI√ìN CRUZADA COMPLETADA\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fd29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Ordenar por MAE (menor es mejor)\n",
    "df_resultados_sorted = df_resultados.sort_values('mae_mean')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP 5 MEJORES CONFIGURACIONES (seg√∫n MAE)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nWindow | Arquitectura    | LR     | MAE (m)         | RMSE (m)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, row in df_resultados_sorted.head(5).iterrows():\n",
    "    print(f\"{row['window_size']:^6} | {row['arquitectura']:^15} | {row['learning_rate']:.4f} | \"\n",
    "          f\"{row['mae_mean']:.4f}¬±{row['mae_std']:.4f} | \"\n",
    "          f\"{row['rmse_mean']:.4f}¬±{row['rmse_std']:.4f}\")\n",
    "\n",
    "# Mejor configuraci√≥n\n",
    "mejor_config = df_resultados_sorted.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üèÜ MEJOR CONFIGURACI√ìN ENCONTRADA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Window size: {mejor_config['window_size']}\")\n",
    "print(f\"  Arquitectura: {mejor_config['arquitectura']}\")\n",
    "print(f\"  Learning rate: {mejor_config['learning_rate']}\")\n",
    "print(f\"  MAE: {mejor_config['mae_mean']:.4f} ¬± {mejor_config['mae_std']:.4f} m\")\n",
    "print(f\"  RMSE: {mejor_config['rmse_mean']:.4f} ¬± {mejor_config['rmse_std']:.4f} m\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57442046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√°metros de entrenamiento\n",
    "EPOCHS = 100        # n√∫mero de veces que recorre todo el dataset (ajust√° seg√∫n rendimiento)\n",
    "BATCH_SIZE = 32     # tama√±o de los lotes de entrenamiento\n",
    "\n",
    "print(f\"Configuraci√≥n de entrenamiento -> EPOCHS: {EPOCHS}, BATCH_SIZE: {BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb689549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENTRENANDO MODELO FINAL CON MEJORES HIPERPAR√ÅMETROS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extraer mejores hiperpar√°metros\n",
    "best_window = int(mejor_config['window_size'])\n",
    "best_arquitectura = eval(mejor_config['arquitectura'])\n",
    "best_lr = (mejor_config['learning_rate'])\n",
    "\n",
    "print(f\"\\nUsando configuraci√≥n √≥ptima:\")\n",
    "print(f\"  Window size: {best_window}\")\n",
    "print(f\"  Arquitectura: {best_arquitectura}\")\n",
    "print(f\"  Learning rate: {best_lr}\")\n",
    "\n",
    "# Crear ventanas con TODO el train+val set (ahora pasando 'features')\n",
    "X_train_final, y_train_final = crear_ventanas(df_train_val, features, window_size=best_window)\n",
    "\n",
    "# Escalar datos\n",
    "scaler_X_final = MinMaxScaler()\n",
    "scaler_y_final = MinMaxScaler()\n",
    "\n",
    "X_train_final_scaled = scaler_X_final.fit_transform(X_train_final)\n",
    "y_train_final_scaled = scaler_y_final.fit_transform(y_train_final.reshape(-1, 1)).flatten()\n",
    "\n",
    "# ‚úÖ Guardar los scalers para usar luego en predicci√≥n\n",
    "import joblib\n",
    "joblib.dump(scaler_X_final, \"scaler_X_final.pkl\")\n",
    "joblib.dump(scaler_y_final, \"scaler_y_final.pkl\")\n",
    "\n",
    "print(\"‚úì Scalers guardados: 'scaler_X_final.pkl' y 'scaler_y_final.pkl'\")\n",
    "\n",
    "# Crear y entrenar modelo final\n",
    "print(\"\\nüîÑ Entrenando modelo final...\")\n",
    "tf.random.set_seed(SEED)\n",
    "modelo_final = crear_modelo(X_train_final_scaled.shape[1], best_arquitectura, best_lr)\n",
    "\n",
    "history_final = modelo_final.fit(\n",
    "    X_train_final_scaled, y_train_final_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,  # Solo para monitorear, no para seleccionar hiperpar√°metros\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Modelo final entrenado exitosamente\")\n",
    "\n",
    "# Guardar modelo\n",
    "modelo_final.save('modelo_final_optimizado.h5')\n",
    "print(\"‚úì Modelo guardado en 'modelo_final_optimizado.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598dd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluaci√≥n final m√≠nima en test set (s√≥lo MAE / RMSE) - versi√≥n compacta para iteraciones r√°pidas\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Crear ventanas para el test set (aseg√∫rate que best_window y modelo/scalers existen)\n",
    "X_test_final, y_test_final = crear_ventanas(df_test, features, window_size=best_window)\n",
    "\n",
    "# Escalar y predecir (usando scalers guardados en entrenamiento final)\n",
    "X_test_final_scaled = scaler_X_final.transform(X_test_final)\n",
    "y_pred_test_scaled = modelo_final.predict(X_test_final_scaled, verbose=0)\n",
    "y_pred_test = scaler_y_final.inverse_transform(y_pred_test_scaled)\n",
    "y_test_real = y_test_final.reshape(-1, 1)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test_real, y_pred_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test_real, y_pred_test))\n",
    "\n",
    "print('\\nüìä EVALUACI√ìN M√çNIMA EN TEST SET:')\n",
    "print(f'  MAE: {mae_test:.6f} m')\n",
    "print(f'  RMSE: {rmse_test:.6f} m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef274c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA DE PREDICCIONES ITERATIVAS / GR√ÅFICOS: eliminado para acelerar iteraciones.\n",
    "# Si necesitas volver a ejecutar an√°lisis/plots, recupera esta funcionalidad desde el backup original.\n",
    "print('Predicciones iterativas y visualizaciones eliminadas en esta copia para acelerar el pipeline.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
