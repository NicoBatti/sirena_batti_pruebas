{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45627558",
   "metadata": {},
   "source": [
    "# Training Model v2.0.2 - F2-Score Optimized (Binary Crossentropy Loss)\n",
    "\n",
    "**Objective:** Maximize F2-Score while maintaining balanced Precision\n",
    "\n",
    "**Key Features:**\n",
    "- Loss function: `binary_crossentropy` (proper probabilistic loss)\n",
    "- Optimization target: `val_f2_score` (primary) + `val_precision` (secondary constraint)\n",
    "- Fixed decision threshold: 0.5 for all metrics\n",
    "- Prevents pathological solutions (high recall, low precision)\n",
    "\n",
    "**Artifacts Saved:**\n",
    "- `Models/model_v2.0.2.h5` - Final trained model\n",
    "- `Models/scaler_v2.0.2.pkl` - Preprocessing scaler\n",
    "\n",
    "**NOT Saved:**\n",
    "- Hyperparameter trial logs\n",
    "- Training history\n",
    "- Checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b1ff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset loaded: 5531 rows, 29 columns\n",
      "âœ“ Target distribution:\n",
      "  - No flood: 2805 (50.7%)\n",
      "  - Flood: 2726 (49.3%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../dataset_rio_2km_week_encoded.csv\")\n",
    "\n",
    "print(f\"âœ“ Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"âœ“ Target distribution:\")\n",
    "print(f\"  - No flood: {(df['seInunda'] == 0).sum()} ({100*(df['seInunda'] == 0).sum()/len(df):.1f}%)\")\n",
    "print(f\"  - Flood: {(df['seInunda'] == 1).sum()} ({100*(df['seInunda'] == 1).sum()/len(df):.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ea6e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Features shape: (5531, 26) (26 features)\n",
      "âœ“ Target shape: (5531,)\n",
      "\n",
      "âœ“ Train set: 4424 samples\n",
      "âœ“ Test set: 1107 samples\n",
      "\n",
      "âœ“ Data preprocessed and ready for training\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPARATION\n",
    "# ============================================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Define features and target\n",
    "FEATURES = [\n",
    "    'hayRioCercano', 'elevacion_m', 'pendiente_grados', \n",
    "    'temperature_2m', 'total_evaporation_sum', \n",
    "    'volumetric_soil_water_layer_1', 'precipitation_week',\n",
    "    # Soil types (One-Hot Encoded)\n",
    "    'tipoDeSuelo_Alfisoles', 'tipoDeSuelo_Aridisoles',\n",
    "    'tipoDeSuelo_Complejo indiferenci', 'tipoDeSuelo_Entisoles',\n",
    "    'tipoDeSuelo_Esteros', 'tipoDeSuelo_Inceptisoles',\n",
    "    'tipoDeSuelo_Lagunas', 'tipoDeSuelo_Medano', 'tipoDeSuelo_Miscelaneas',\n",
    "    'tipoDeSuelo_Molisoles', 'tipoDeSuelo_Rio', 'tipoDeSuelo_Roca',\n",
    "    'tipoDeSuelo_Salinas', 'tipoDeSuelo_Ultisoles', 'tipoDeSuelo_Vertisoles',\n",
    "    # Seasons (One-Hot Encoded)\n",
    "    'estacion_invierno', 'estacion_otoÃ±o', 'estacion_primavera', 'estacion_verano'\n",
    "]\n",
    "TARGET = 'seInunda'\n",
    "\n",
    "# Extract features and target\n",
    "X = df[FEATURES].values.astype(np.float32)\n",
    "y = df[TARGET].values.astype(np.int32)\n",
    "\n",
    "print(f\"âœ“ Features shape: {X.shape} ({len(FEATURES)} features)\")\n",
    "print(f\"âœ“ Target shape: {y.shape}\")\n",
    "\n",
    "# Train/test split with stratification (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"âœ“ Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Standardize features (fit on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nâœ“ Data preprocessed and ready for training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48554c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Custom F2-Score metric defined (for evaluation only)\n",
      "âœ“ Loss function will be: binary_crossentropy\n",
      "âœ“ All metrics use fixed threshold: 0.5\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CUSTOM F2-SCORE METRIC (METRIC ONLY, NOT LOSS)\n",
    "# ============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "\n",
    "class F2Score(keras.metrics.Metric):\n",
    "    \"\"\"\n",
    "    Custom F2-Score metric for Keras.\n",
    "    \n",
    "    F2 = (1 + Î²Â²) Ã— (precision Ã— recall) / (Î²Â² Ã— precision + recall), where Î²=2\n",
    "    \n",
    "    This gives more weight to recall (80%) than precision (20%).\n",
    "    \n",
    "    IMPORTANT: This is a METRIC only, not used as a loss function.\n",
    "    The model uses binary_crossentropy as loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, name='f2_score', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.precision_metric = keras.metrics.Precision(thresholds=threshold)\n",
    "        self.recall_metric = keras.metrics.Recall(thresholds=threshold)\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision_metric.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall_metric.update_state(y_true, y_pred, sample_weight)\n",
    "    \n",
    "    def result(self):\n",
    "        precision = self.precision_metric.result()\n",
    "        recall = self.recall_metric.result()\n",
    "        \n",
    "        # F2 = 5 * (precision * recall) / (4 * precision + recall)\n",
    "        numerator = 5.0 * precision * recall\n",
    "        denominator = 4.0 * precision + recall\n",
    "        \n",
    "        return tf.where(\n",
    "            tf.equal(denominator, 0),\n",
    "            0.0,\n",
    "            numerator / denominator\n",
    "        )\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.precision_metric.reset_state()\n",
    "        self.recall_metric.reset_state()\n",
    "\n",
    "\n",
    "print(\"âœ“ Custom F2-Score metric defined (for evaluation only)\")\n",
    "print(\"âœ“ Loss function will be: binary_crossentropy\")\n",
    "print(\"âœ“ All metrics use fixed threshold: 0.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4713b1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER SEARCH CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "âœ“ Tuner configured:\n",
      "  - Loss: binary_crossentropy (NOT F2 loss)\n",
      "  - Primary objective: Maximize val_f2_score\n",
      "  - Secondary objective: Maximize val_precision\n",
      "  - This prevents models with recallâ‰ˆ1.0 and precisionâ‰ˆ0.0\n",
      "  - Max trials: 30\n",
      "  - Executions per trial: 2\n",
      "\n",
      "âœ“ Search space:\n",
      "  - Layers: 2-5\n",
      "  - Units per layer: 32-256 (step 32)\n",
      "  - Dropout: 0.1-0.5 (step 0.1)\n",
      "  - Learning rate: [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
      "  - Activation: [relu, elu, swish]\n",
      "\n",
      "âœ“ All metrics use fixed threshold: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COMPU\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HYPERPARAMETER SEARCH WITH KERAS TUNER\n",
    "# ============================================================================\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Build a model with tunable hyperparameters.\n",
    "    \n",
    "    IMPORTANT CHANGES:\n",
    "    - Loss: binary_crossentropy (NOT F2 loss)\n",
    "    - Metrics: All use threshold=0.5\n",
    "    - Tuner optimizes: val_f2_score (primary) + val_precision (constraint)\n",
    "    \n",
    "    Search space:\n",
    "    - Number of layers: 2-5\n",
    "    - Units per layer: 32-256\n",
    "    - Dropout rates: 0.1-0.5\n",
    "    - Learning rate: 1e-4 to 1e-2\n",
    "    - Activation functions: relu, elu, swish\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
    "    \n",
    "    # Tunable number of hidden layers\n",
    "    n_layers = hp.Int('n_layers', min_value=2, max_value=5, step=1)\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        # Tunable units per layer\n",
    "        units = hp.Int(f'units_{i}', min_value=32, max_value=256, step=32)\n",
    "        \n",
    "        # Tunable activation\n",
    "        activation = hp.Choice(f'activation_{i}', values=['relu', 'elu', 'swish'])\n",
    "        \n",
    "        # Add dense layer with L2 regularization\n",
    "        model.add(layers.Dense(\n",
    "            units, \n",
    "            activation=activation,\n",
    "            kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "        ))\n",
    "        \n",
    "        # Tunable dropout rate\n",
    "        dropout = hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Tunable learning rate\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-4, 5e-4, 1e-3, 5e-3, 1e-2])\n",
    "    \n",
    "    # Compile with BINARY CROSSENTROPY loss (proper probabilistic loss)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',  # Standard loss for binary classification\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.Precision(name='precision', thresholds=0.5),  # Fixed threshold\n",
    "            keras.metrics.Recall(name='recall', thresholds=0.5),        # Fixed threshold\n",
    "            F2Score(name='f2_score', threshold=0.5)                     # Fixed threshold\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Configure the tuner with dual objectives\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER SEARCH CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Primary objective: Maximize F2-Score\n",
    "# Secondary constraint: Maintain reasonable precision\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=[\n",
    "        kt.Objective('val_f2_score', direction='max'),      # Primary: maximize F2\n",
    "        kt.Objective('val_precision', direction='max')      # Secondary: maintain precision\n",
    "    ],\n",
    "    max_trials=30,\n",
    "    executions_per_trial=2,\n",
    "    directory='../hyperparameter_tuning',\n",
    "    project_name='flood_prediction_crossentropy_v2.0.2',\n",
    "    overwrite=False\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Tuner configured:\")\n",
    "print(f\"  - Loss: binary_crossentropy (NOT F2 loss)\")\n",
    "print(f\"  - Primary objective: Maximize val_f2_score\")\n",
    "print(f\"  - Secondary objective: Maximize val_precision\")\n",
    "print(f\"  - This prevents models with recallâ‰ˆ1.0 and precisionâ‰ˆ0.0\")\n",
    "print(f\"  - Max trials: 30\")\n",
    "print(f\"  - Executions per trial: 2\")\n",
    "print(f\"\\nâœ“ Search space:\")\n",
    "print(f\"  - Layers: 2-5\")\n",
    "print(f\"  - Units per layer: 32-256 (step 32)\")\n",
    "print(f\"  - Dropout: 0.1-0.5 (step 0.1)\")\n",
    "print(f\"  - Learning rate: [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\")\n",
    "print(f\"  - Activation: [relu, elu, swish]\")\n",
    "print(f\"\\nâœ“ All metrics use fixed threshold: 0.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ddb2a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SEARCHING FOR BEST HYPERPARAMETERS\n",
      "======================================================================\n",
      "âš ï¸  This may take several minutes to hours depending on hardware...\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\COMPU\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\COMPU\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\COMPU\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ† BEST HYPERPARAMETERS FOUND\n",
      "======================================================================\n",
      "\n",
      "Number of layers: 3\n",
      "  Layer 1:\n",
      "    - Units: 192\n",
      "    - Activation: relu\n",
      "    - Dropout: 0.2\n",
      "  Layer 2:\n",
      "    - Units: 128\n",
      "    - Activation: relu\n",
      "    - Dropout: 0.4\n",
      "  Layer 3:\n",
      "    - Units: 224\n",
      "    - Activation: swish\n",
      "    - Dropout: 0.1\n",
      "\n",
      "Learning rate: 0.005\n",
      "\n",
      "======================================================================\n",
      "TOP 3 MODELS BY F2-SCORE\n",
      "======================================================================\n",
      "\n",
      "Rank 1:\n",
      "  F2-Score: 0.8396\n",
      "  Precision: 0.7023\n",
      "  Recall: 0.8834\n",
      "\n",
      "Rank 2:\n",
      "  F2-Score: 0.8207\n",
      "  Precision: 0.7184\n",
      "  Recall: 0.8510\n",
      "\n",
      "Rank 3:\n",
      "  F2-Score: 0.8012\n",
      "  Precision: 0.7307\n",
      "  Recall: 0.8210\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HYPERPARAMETER SEARCH EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SEARCHING FOR BEST HYPERPARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "print(\"âš ï¸  This may take several minutes to hours depending on hardware...\")\n",
    "print()\n",
    "\n",
    "# Configure early stopping for each trial\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss (binary crossentropy)\n",
    "    mode='min',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ† BEST HYPERPARAMETERS FOUND\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nNumber of layers: {best_hps.get('n_layers')}\")\n",
    "for i in range(best_hps.get('n_layers')):\n",
    "    print(f\"  Layer {i+1}:\")\n",
    "    print(f\"    - Units: {best_hps.get(f'units_{i}')}\")\n",
    "    print(f\"    - Activation: {best_hps.get(f'activation_{i}')}\")\n",
    "    print(f\"    - Dropout: {best_hps.get(f'dropout_{i}')}\")\n",
    "print(f\"\\nLearning rate: {best_hps.get('learning_rate')}\")\n",
    "\n",
    "# Get best models summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 3 MODELS BY F2-SCORE\")\n",
    "print(\"=\"*70)\n",
    "for idx, model_trial in enumerate(tuner.oracle.get_best_trials(num_trials=3), 1):\n",
    "    f2_score = model_trial.metrics.get_best_value('val_f2_score')\n",
    "    precision = model_trial.metrics.get_best_value('val_precision')\n",
    "    recall = model_trial.metrics.get_best_value('val_recall')\n",
    "    print(f\"\\nRank {idx}:\")\n",
    "    print(f\"  F2-Score: {f2_score:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5d8856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING FINAL MODEL\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Model Architecture (with best hyperparameters):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,184</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">28,896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            â”‚         \u001b[38;5;34m5,184\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m24,704\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m)            â”‚        \u001b[38;5;34m28,896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚           \u001b[38;5;34m225\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,009</span> (230.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m59,009\u001b[0m (230.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,009</span> (230.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m59,009\u001b[0m (230.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training...\n",
      "Epoch 1/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6880 - f2_score: 0.6886 - loss: 0.7859 - precision: 0.6817 - recall: 0.6903 - val_accuracy: 0.7288 - val_f2_score: 0.8118 - val_loss: 0.6511 - val_precision: 0.6764 - val_recall: 0.8545\n",
      "Epoch 2/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6880 - f2_score: 0.6886 - loss: 0.7859 - precision: 0.6817 - recall: 0.6903 - val_accuracy: 0.7288 - val_f2_score: 0.8118 - val_loss: 0.6511 - val_precision: 0.6764 - val_recall: 0.8545\n",
      "Epoch 2/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7177 - f2_score: 0.7460 - loss: 0.6378 - precision: 0.6962 - recall: 0.7596 - val_accuracy: 0.7299 - val_f2_score: 0.7865 - val_loss: 0.5878 - val_precision: 0.6895 - val_recall: 0.8152\n",
      "Epoch 3/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7177 - f2_score: 0.7460 - loss: 0.6378 - precision: 0.6962 - recall: 0.7596 - val_accuracy: 0.7299 - val_f2_score: 0.7865 - val_loss: 0.5878 - val_precision: 0.6895 - val_recall: 0.8152\n",
      "Epoch 3/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7211 - f2_score: 0.7584 - loss: 0.6023 - precision: 0.6947 - recall: 0.7762 - val_accuracy: 0.7299 - val_f2_score: 0.8165 - val_loss: 0.5758 - val_precision: 0.6757 - val_recall: 0.8614\n",
      "Epoch 4/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7211 - f2_score: 0.7584 - loss: 0.6023 - precision: 0.6947 - recall: 0.7762 - val_accuracy: 0.7299 - val_f2_score: 0.8165 - val_loss: 0.5758 - val_precision: 0.6757 - val_recall: 0.8614\n",
      "Epoch 4/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7253 - f2_score: 0.7670 - loss: 0.5964 - precision: 0.6962 - recall: 0.7871 - val_accuracy: 0.7537 - val_f2_score: 0.7785 - val_loss: 0.5652 - val_precision: 0.7282 - val_recall: 0.7921\n",
      "Epoch 5/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7253 - f2_score: 0.7670 - loss: 0.5964 - precision: 0.6962 - recall: 0.7871 - val_accuracy: 0.7537 - val_f2_score: 0.7785 - val_loss: 0.5652 - val_precision: 0.7282 - val_recall: 0.7921\n",
      "Epoch 5/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7270 - f2_score: 0.7586 - loss: 0.5823 - precision: 0.7031 - recall: 0.7739 - val_accuracy: 0.7379 - val_f2_score: 0.7239 - val_loss: 0.5665 - val_precision: 0.7376 - val_recall: 0.7206\n",
      "Epoch 6/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7270 - f2_score: 0.7586 - loss: 0.5823 - precision: 0.7031 - recall: 0.7739 - val_accuracy: 0.7379 - val_f2_score: 0.7239 - val_loss: 0.5665 - val_precision: 0.7376 - val_recall: 0.7206\n",
      "Epoch 6/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7228 - f2_score: 0.7608 - loss: 0.5847 - precision: 0.6958 - recall: 0.7790 - val_accuracy: 0.7435 - val_f2_score: 0.7387 - val_loss: 0.5642 - val_precision: 0.7373 - val_recall: 0.7390\n",
      "Epoch 7/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7228 - f2_score: 0.7608 - loss: 0.5847 - precision: 0.6958 - recall: 0.7790 - val_accuracy: 0.7435 - val_f2_score: 0.7387 - val_loss: 0.5642 - val_precision: 0.7373 - val_recall: 0.7390\n",
      "Epoch 7/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7237 - f2_score: 0.7584 - loss: 0.5783 - precision: 0.6983 - recall: 0.7750 - val_accuracy: 0.7401 - val_f2_score: 0.7973 - val_loss: 0.5577 - val_precision: 0.6979 - val_recall: 0.8268\n",
      "Epoch 8/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7237 - f2_score: 0.7584 - loss: 0.5783 - precision: 0.6983 - recall: 0.7750 - val_accuracy: 0.7401 - val_f2_score: 0.7973 - val_loss: 0.5577 - val_precision: 0.6979 - val_recall: 0.8268\n",
      "Epoch 8/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7285 - f2_score: 0.7784 - loss: 0.5739 - precision: 0.6947 - recall: 0.8025 - val_accuracy: 0.7333 - val_f2_score: 0.7845 - val_loss: 0.5612 - val_precision: 0.6950 - val_recall: 0.8106\n",
      "Epoch 9/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7285 - f2_score: 0.7784 - loss: 0.5739 - precision: 0.6947 - recall: 0.8025 - val_accuracy: 0.7333 - val_f2_score: 0.7845 - val_loss: 0.5612 - val_precision: 0.6950 - val_recall: 0.8106\n",
      "Epoch 9/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7321 - f2_score: 0.7629 - loss: 0.5714 - precision: 0.7082 - recall: 0.7779 - val_accuracy: 0.7401 - val_f2_score: 0.8301 - val_loss: 0.5645 - val_precision: 0.6822 - val_recall: 0.8776\n",
      "Epoch 10/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7321 - f2_score: 0.7629 - loss: 0.5714 - precision: 0.7082 - recall: 0.7779 - val_accuracy: 0.7401 - val_f2_score: 0.8301 - val_loss: 0.5645 - val_precision: 0.6822 - val_recall: 0.8776\n",
      "Epoch 10/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7350 - f2_score: 0.7700 - loss: 0.5720 - precision: 0.7084 - recall: 0.7871 - val_accuracy: 0.7593 - val_f2_score: 0.7581 - val_loss: 0.5466 - val_precision: 0.7511 - val_recall: 0.7598\n",
      "Epoch 11/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7350 - f2_score: 0.7700 - loss: 0.5720 - precision: 0.7084 - recall: 0.7871 - val_accuracy: 0.7593 - val_f2_score: 0.7581 - val_loss: 0.5466 - val_precision: 0.7511 - val_recall: 0.7598\n",
      "Epoch 11/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7324 - f2_score: 0.7719 - loss: 0.5695 - precision: 0.7037 - recall: 0.7911 - val_accuracy: 0.7412 - val_f2_score: 0.7731 - val_loss: 0.5562 - val_precision: 0.7125 - val_recall: 0.7898\n",
      "Epoch 12/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7324 - f2_score: 0.7719 - loss: 0.5695 - precision: 0.7037 - recall: 0.7911 - val_accuracy: 0.7412 - val_f2_score: 0.7731 - val_loss: 0.5562 - val_precision: 0.7125 - val_recall: 0.7898\n",
      "Epoch 12/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7318 - f2_score: 0.7709 - loss: 0.5687 - precision: 0.7034 - recall: 0.7899 - val_accuracy: 0.7571 - val_f2_score: 0.8163 - val_loss: 0.5497 - val_precision: 0.7112 - val_recall: 0.8476\n",
      "Epoch 13/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7318 - f2_score: 0.7709 - loss: 0.5687 - precision: 0.7034 - recall: 0.7899 - val_accuracy: 0.7571 - val_f2_score: 0.8163 - val_loss: 0.5497 - val_precision: 0.7112 - val_recall: 0.8476\n",
      "Epoch 13/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7361 - f2_score: 0.7730 - loss: 0.5701 - precision: 0.7084 - recall: 0.7911 - val_accuracy: 0.7288 - val_f2_score: 0.8264 - val_loss: 0.5593 - val_precision: 0.6702 - val_recall: 0.8776\n",
      "Epoch 14/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7361 - f2_score: 0.7730 - loss: 0.5701 - precision: 0.7084 - recall: 0.7911 - val_accuracy: 0.7288 - val_f2_score: 0.8264 - val_loss: 0.5593 - val_precision: 0.6702 - val_recall: 0.8776\n",
      "Epoch 14/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7338 - f2_score: 0.7835 - loss: 0.5650 - precision: 0.6996 - recall: 0.8077 - val_accuracy: 0.7492 - val_f2_score: 0.7802 - val_loss: 0.5486 - val_precision: 0.7203 - val_recall: 0.7968\n",
      "Epoch 15/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7338 - f2_score: 0.7835 - loss: 0.5650 - precision: 0.6996 - recall: 0.8077 - val_accuracy: 0.7492 - val_f2_score: 0.7802 - val_loss: 0.5486 - val_precision: 0.7203 - val_recall: 0.7968\n",
      "Epoch 15/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7307 - f2_score: 0.7764 - loss: 0.5706 - precision: 0.6989 - recall: 0.7985 - val_accuracy: 0.7435 - val_f2_score: 0.7753 - val_loss: 0.5578 - val_precision: 0.7146 - val_recall: 0.7921\n",
      "Epoch 16/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7307 - f2_score: 0.7764 - loss: 0.5706 - precision: 0.6989 - recall: 0.7985 - val_accuracy: 0.7435 - val_f2_score: 0.7753 - val_loss: 0.5578 - val_precision: 0.7146 - val_recall: 0.7921\n",
      "Epoch 16/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7330 - f2_score: 0.7805 - loss: 0.5664 - precision: 0.6999 - recall: 0.8037 - val_accuracy: 0.7537 - val_f2_score: 0.8062 - val_loss: 0.5510 - val_precision: 0.7120 - val_recall: 0.8337\n",
      "Epoch 17/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7330 - f2_score: 0.7805 - loss: 0.5664 - precision: 0.6999 - recall: 0.8037 - val_accuracy: 0.7537 - val_f2_score: 0.8062 - val_loss: 0.5510 - val_precision: 0.7120 - val_recall: 0.8337\n",
      "Epoch 17/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7409 - f2_score: 0.7783 - loss: 0.5599 - precision: 0.7124 - recall: 0.7968 - val_accuracy: 0.7458 - val_f2_score: 0.8275 - val_loss: 0.5432 - val_precision: 0.6905 - val_recall: 0.8707\n",
      "Epoch 18/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7409 - f2_score: 0.7783 - loss: 0.5599 - precision: 0.7124 - recall: 0.7968 - val_accuracy: 0.7458 - val_f2_score: 0.8275 - val_loss: 0.5432 - val_precision: 0.6905 - val_recall: 0.8707\n",
      "Epoch 18/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7389 - f2_score: 0.7727 - loss: 0.5612 - precision: 0.7127 - recall: 0.7894 - val_accuracy: 0.7412 - val_f2_score: 0.7461 - val_loss: 0.5555 - val_precision: 0.7287 - val_recall: 0.7506\n",
      "Epoch 19/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7389 - f2_score: 0.7727 - loss: 0.5612 - precision: 0.7127 - recall: 0.7894 - val_accuracy: 0.7412 - val_f2_score: 0.7461 - val_loss: 0.5555 - val_precision: 0.7287 - val_recall: 0.7506\n",
      "Epoch 19/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7316 - f2_score: 0.7720 - loss: 0.5601 - precision: 0.7024 - recall: 0.7916 - val_accuracy: 0.7458 - val_f2_score: 0.7410 - val_loss: 0.5487 - val_precision: 0.7396 - val_recall: 0.7413\n",
      "Epoch 20/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7316 - f2_score: 0.7720 - loss: 0.5601 - precision: 0.7024 - recall: 0.7916 - val_accuracy: 0.7458 - val_f2_score: 0.7410 - val_loss: 0.5487 - val_precision: 0.7396 - val_recall: 0.7413\n",
      "Epoch 20/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7431 - f2_score: 0.7678 - loss: 0.5546 - precision: 0.7219 - recall: 0.7802 - val_accuracy: 0.7435 - val_f2_score: 0.7548 - val_loss: 0.5672 - val_precision: 0.7269 - val_recall: 0.7621\n",
      "Epoch 21/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7431 - f2_score: 0.7678 - loss: 0.5546 - precision: 0.7219 - recall: 0.7802 - val_accuracy: 0.7435 - val_f2_score: 0.7548 - val_loss: 0.5672 - val_precision: 0.7269 - val_recall: 0.7621\n",
      "Epoch 21/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7389 - f2_score: 0.7719 - loss: 0.5591 - precision: 0.7131 - recall: 0.7882 - val_accuracy: 0.7514 - val_f2_score: 0.8070 - val_loss: 0.5451 - val_precision: 0.7084 - val_recall: 0.8360\n",
      "Epoch 22/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7389 - f2_score: 0.7719 - loss: 0.5591 - precision: 0.7131 - recall: 0.7882 - val_accuracy: 0.7514 - val_f2_score: 0.8070 - val_loss: 0.5451 - val_precision: 0.7084 - val_recall: 0.8360\n",
      "Epoch 22/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7477 - f2_score: 0.7812 - loss: 0.5570 - precision: 0.7208 - recall: 0.7979 - val_accuracy: 0.7525 - val_f2_score: 0.7967 - val_loss: 0.5402 - val_precision: 0.7157 - val_recall: 0.8199\n",
      "Epoch 23/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7477 - f2_score: 0.7812 - loss: 0.5570 - precision: 0.7208 - recall: 0.7979 - val_accuracy: 0.7525 - val_f2_score: 0.7967 - val_loss: 0.5402 - val_precision: 0.7157 - val_recall: 0.8199\n",
      "Epoch 23/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7448 - f2_score: 0.7842 - loss: 0.5574 - precision: 0.7149 - recall: 0.8037 - val_accuracy: 0.7299 - val_f2_score: 0.7789 - val_loss: 0.5528 - val_precision: 0.6932 - val_recall: 0.8037\n",
      "Epoch 24/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7448 - f2_score: 0.7842 - loss: 0.5574 - precision: 0.7149 - recall: 0.8037 - val_accuracy: 0.7299 - val_f2_score: 0.7789 - val_loss: 0.5528 - val_precision: 0.6932 - val_recall: 0.8037\n",
      "Epoch 24/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7409 - f2_score: 0.7780 - loss: 0.5597 - precision: 0.7126 - recall: 0.7962 - val_accuracy: 0.7492 - val_f2_score: 0.8330 - val_loss: 0.5590 - val_precision: 0.6922 - val_recall: 0.8776\n",
      "Epoch 25/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7409 - f2_score: 0.7780 - loss: 0.5597 - precision: 0.7126 - recall: 0.7962 - val_accuracy: 0.7492 - val_f2_score: 0.8330 - val_loss: 0.5590 - val_precision: 0.6922 - val_recall: 0.8776\n",
      "Epoch 25/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7423 - f2_score: 0.7792 - loss: 0.5517 - precision: 0.7140 - recall: 0.7974 - val_accuracy: 0.7503 - val_f2_score: 0.8186 - val_loss: 0.5436 - val_precision: 0.7008 - val_recall: 0.8545\n",
      "Epoch 26/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7423 - f2_score: 0.7792 - loss: 0.5517 - precision: 0.7140 - recall: 0.7974 - val_accuracy: 0.7503 - val_f2_score: 0.8186 - val_loss: 0.5436 - val_precision: 0.7008 - val_recall: 0.8545\n",
      "Epoch 26/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7485 - f2_score: 0.7776 - loss: 0.5510 - precision: 0.7242 - recall: 0.7922 - val_accuracy: 0.7661 - val_f2_score: 0.8010 - val_loss: 0.5465 - val_precision: 0.7335 - val_recall: 0.8199\n",
      "Epoch 27/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7485 - f2_score: 0.7776 - loss: 0.5510 - precision: 0.7242 - recall: 0.7922 - val_accuracy: 0.7661 - val_f2_score: 0.8010 - val_loss: 0.5465 - val_precision: 0.7335 - val_recall: 0.8199\n",
      "Epoch 27/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7437 - f2_score: 0.7834 - loss: 0.5561 - precision: 0.7136 - recall: 0.8031 - val_accuracy: 0.7503 - val_f2_score: 0.7898 - val_loss: 0.5408 - val_precision: 0.7163 - val_recall: 0.8106\n",
      "Epoch 28/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7437 - f2_score: 0.7834 - loss: 0.5561 - precision: 0.7136 - recall: 0.8031 - val_accuracy: 0.7503 - val_f2_score: 0.7898 - val_loss: 0.5408 - val_precision: 0.7163 - val_recall: 0.8106\n",
      "Epoch 28/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7381 - f2_score: 0.7740 - loss: 0.5555 - precision: 0.7107 - recall: 0.7916 - val_accuracy: 0.7582 - val_f2_score: 0.7625 - val_loss: 0.5437 - val_precision: 0.7461 - val_recall: 0.7667\n",
      "Epoch 29/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7381 - f2_score: 0.7740 - loss: 0.5555 - precision: 0.7107 - recall: 0.7916 - val_accuracy: 0.7582 - val_f2_score: 0.7625 - val_loss: 0.5437 - val_precision: 0.7461 - val_recall: 0.7667\n",
      "Epoch 29/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7448 - f2_score: 0.7849 - loss: 0.5454 - precision: 0.7144 - recall: 0.8048 - val_accuracy: 0.7446 - val_f2_score: 0.7927 - val_loss: 0.5508 - val_precision: 0.7066 - val_recall: 0.8176\n",
      "Epoch 30/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7448 - f2_score: 0.7849 - loss: 0.5454 - precision: 0.7144 - recall: 0.8048 - val_accuracy: 0.7446 - val_f2_score: 0.7927 - val_loss: 0.5508 - val_precision: 0.7066 - val_recall: 0.8176\n",
      "Epoch 30/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7482 - f2_score: 0.7845 - loss: 0.5477 - precision: 0.7197 - recall: 0.8025 - val_accuracy: 0.7514 - val_f2_score: 0.7636 - val_loss: 0.5460 - val_precision: 0.7341 - val_recall: 0.7714\n",
      "Epoch 31/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7482 - f2_score: 0.7845 - loss: 0.5477 - precision: 0.7197 - recall: 0.8025 - val_accuracy: 0.7514 - val_f2_score: 0.7636 - val_loss: 0.5460 - val_precision: 0.7341 - val_recall: 0.7714\n",
      "Epoch 31/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7471 - f2_score: 0.7837 - loss: 0.5501 - precision: 0.7185 - recall: 0.8019 - val_accuracy: 0.7525 - val_f2_score: 0.8103 - val_loss: 0.5465 - val_precision: 0.7082 - val_recall: 0.8406\n",
      "Epoch 32/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7471 - f2_score: 0.7837 - loss: 0.5501 - precision: 0.7185 - recall: 0.8019 - val_accuracy: 0.7525 - val_f2_score: 0.8103 - val_loss: 0.5465 - val_precision: 0.7082 - val_recall: 0.8406\n",
      "Epoch 32/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7426 - f2_score: 0.7835 - loss: 0.5480 - precision: 0.7120 - recall: 0.8037 - val_accuracy: 0.7492 - val_f2_score: 0.8197 - val_loss: 0.5385 - val_precision: 0.6987 - val_recall: 0.8568\n",
      "Epoch 33/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7426 - f2_score: 0.7835 - loss: 0.5480 - precision: 0.7120 - recall: 0.8037 - val_accuracy: 0.7492 - val_f2_score: 0.8197 - val_loss: 0.5385 - val_precision: 0.6987 - val_recall: 0.8568\n",
      "Epoch 33/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7437 - f2_score: 0.7804 - loss: 0.5528 - precision: 0.7154 - recall: 0.7985 - val_accuracy: 0.7424 - val_f2_score: 0.8175 - val_loss: 0.5515 - val_precision: 0.6909 - val_recall: 0.8568\n",
      "Epoch 34/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7437 - f2_score: 0.7804 - loss: 0.5528 - precision: 0.7154 - recall: 0.7985 - val_accuracy: 0.7424 - val_f2_score: 0.8175 - val_loss: 0.5515 - val_precision: 0.6909 - val_recall: 0.8568\n",
      "Epoch 34/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7474 - f2_score: 0.7865 - loss: 0.5557 - precision: 0.7173 - recall: 0.8060 - val_accuracy: 0.7458 - val_f2_score: 0.7945 - val_loss: 0.5474 - val_precision: 0.7072 - val_recall: 0.8199\n",
      "Epoch 35/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7474 - f2_score: 0.7865 - loss: 0.5557 - precision: 0.7173 - recall: 0.8060 - val_accuracy: 0.7458 - val_f2_score: 0.7945 - val_loss: 0.5474 - val_precision: 0.7072 - val_recall: 0.8199\n",
      "Epoch 35/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7463 - f2_score: 0.7900 - loss: 0.5504 - precision: 0.7136 - recall: 0.8117 - val_accuracy: 0.7514 - val_f2_score: 0.8352 - val_loss: 0.5466 - val_precision: 0.6940 - val_recall: 0.8799\n",
      "Epoch 36/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7463 - f2_score: 0.7900 - loss: 0.5504 - precision: 0.7136 - recall: 0.8117 - val_accuracy: 0.7514 - val_f2_score: 0.8352 - val_loss: 0.5466 - val_precision: 0.6940 - val_recall: 0.8799\n",
      "Epoch 36/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7437 - f2_score: 0.7850 - loss: 0.5494 - precision: 0.7128 - recall: 0.8054 - val_accuracy: 0.7548 - val_f2_score: 0.7897 - val_loss: 0.5431 - val_precision: 0.7231 - val_recall: 0.8083\n",
      "Epoch 37/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7437 - f2_score: 0.7850 - loss: 0.5494 - precision: 0.7128 - recall: 0.8054 - val_accuracy: 0.7548 - val_f2_score: 0.7897 - val_loss: 0.5431 - val_precision: 0.7231 - val_recall: 0.8083\n",
      "Epoch 37/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7423 - f2_score: 0.7842 - loss: 0.5493 - precision: 0.7112 - recall: 0.8048 - val_accuracy: 0.7469 - val_f2_score: 0.8205 - val_loss: 0.5493 - val_precision: 0.6953 - val_recall: 0.8591\n",
      "Epoch 38/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7423 - f2_score: 0.7842 - loss: 0.5493 - precision: 0.7112 - recall: 0.8048 - val_accuracy: 0.7469 - val_f2_score: 0.8205 - val_loss: 0.5493 - val_precision: 0.6953 - val_recall: 0.8591\n",
      "Epoch 38/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7511 - f2_score: 0.7888 - loss: 0.5492 - precision: 0.7214 - recall: 0.8077 - val_accuracy: 0.7356 - val_f2_score: 0.7666 - val_loss: 0.5502 - val_precision: 0.7077 - val_recall: 0.7829\n",
      "Epoch 39/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7511 - f2_score: 0.7888 - loss: 0.5492 - precision: 0.7214 - recall: 0.8077 - val_accuracy: 0.7356 - val_f2_score: 0.7666 - val_loss: 0.5502 - val_precision: 0.7077 - val_recall: 0.7829\n",
      "Epoch 39/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7446 - f2_score: 0.7887 - loss: 0.5547 - precision: 0.7119 - recall: 0.8105 - val_accuracy: 0.7525 - val_f2_score: 0.8178 - val_loss: 0.5486 - val_precision: 0.7042 - val_recall: 0.8522\n",
      "Epoch 40/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7446 - f2_score: 0.7887 - loss: 0.5547 - precision: 0.7119 - recall: 0.8105 - val_accuracy: 0.7525 - val_f2_score: 0.8178 - val_loss: 0.5486 - val_precision: 0.7042 - val_recall: 0.8522\n",
      "Epoch 40/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7519 - f2_score: 0.7864 - loss: 0.5456 - precision: 0.7241 - recall: 0.8037 - val_accuracy: 0.7446 - val_f2_score: 0.7988 - val_loss: 0.5393 - val_precision: 0.7033 - val_recall: 0.8268\n",
      "Epoch 41/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7519 - f2_score: 0.7864 - loss: 0.5456 - precision: 0.7241 - recall: 0.8037 - val_accuracy: 0.7446 - val_f2_score: 0.7988 - val_loss: 0.5393 - val_precision: 0.7033 - val_recall: 0.8268\n",
      "Epoch 41/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7471 - f2_score: 0.7841 - loss: 0.5456 - precision: 0.7182 - recall: 0.8025 - val_accuracy: 0.7650 - val_f2_score: 0.7976 - val_loss: 0.5344 - val_precision: 0.7339 - val_recall: 0.8152\n",
      "Epoch 42/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7471 - f2_score: 0.7841 - loss: 0.5456 - precision: 0.7182 - recall: 0.8025 - val_accuracy: 0.7650 - val_f2_score: 0.7976 - val_loss: 0.5344 - val_precision: 0.7339 - val_recall: 0.8152\n",
      "Epoch 42/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7485 - f2_score: 0.7846 - loss: 0.5428 - precision: 0.7201 - recall: 0.8025 - val_accuracy: 0.7605 - val_f2_score: 0.8204 - val_loss: 0.5302 - val_precision: 0.7137 - val_recall: 0.8522\n",
      "Epoch 43/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7485 - f2_score: 0.7846 - loss: 0.5428 - precision: 0.7201 - recall: 0.8025 - val_accuracy: 0.7605 - val_f2_score: 0.8204 - val_loss: 0.5302 - val_precision: 0.7137 - val_recall: 0.8522\n",
      "Epoch 43/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7477 - f2_score: 0.7843 - loss: 0.5483 - precision: 0.7190 - recall: 0.8025 - val_accuracy: 0.7469 - val_f2_score: 0.8278 - val_loss: 0.5384 - val_precision: 0.6917 - val_recall: 0.8707\n",
      "Epoch 44/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7477 - f2_score: 0.7843 - loss: 0.5483 - precision: 0.7190 - recall: 0.8025 - val_accuracy: 0.7469 - val_f2_score: 0.8278 - val_loss: 0.5384 - val_precision: 0.6917 - val_recall: 0.8707\n",
      "Epoch 44/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7545 - f2_score: 0.7971 - loss: 0.5459 - precision: 0.7215 - recall: 0.8185 - val_accuracy: 0.7458 - val_f2_score: 0.7915 - val_loss: 0.5532 - val_precision: 0.7088 - val_recall: 0.8152\n",
      "Epoch 45/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7545 - f2_score: 0.7971 - loss: 0.5459 - precision: 0.7215 - recall: 0.8185 - val_accuracy: 0.7458 - val_f2_score: 0.7915 - val_loss: 0.5532 - val_precision: 0.7088 - val_recall: 0.8152\n",
      "Epoch 45/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7443 - f2_score: 0.7832 - loss: 0.5471 - precision: 0.7146 - recall: 0.8025 - val_accuracy: 0.7458 - val_f2_score: 0.7760 - val_loss: 0.5418 - val_precision: 0.7176 - val_recall: 0.7921\n",
      "Epoch 46/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7443 - f2_score: 0.7832 - loss: 0.5471 - precision: 0.7146 - recall: 0.8025 - val_accuracy: 0.7458 - val_f2_score: 0.7760 - val_loss: 0.5418 - val_precision: 0.7176 - val_recall: 0.7921\n",
      "Epoch 46/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7471 - f2_score: 0.7776 - loss: 0.5436 - precision: 0.7221 - recall: 0.7928 - val_accuracy: 0.7480 - val_f2_score: 0.7752 - val_loss: 0.5412 - val_precision: 0.7215 - val_recall: 0.7898\n",
      "Epoch 47/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7471 - f2_score: 0.7776 - loss: 0.5436 - precision: 0.7221 - recall: 0.7928 - val_accuracy: 0.7480 - val_f2_score: 0.7752 - val_loss: 0.5412 - val_precision: 0.7215 - val_recall: 0.7898\n",
      "Epoch 47/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7471 - f2_score: 0.7845 - loss: 0.5446 - precision: 0.7180 - recall: 0.8031 - val_accuracy: 0.7503 - val_f2_score: 0.7664 - val_loss: 0.5486 - val_precision: 0.7304 - val_recall: 0.7760\n",
      "Epoch 48/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7471 - f2_score: 0.7845 - loss: 0.5446 - precision: 0.7180 - recall: 0.8031 - val_accuracy: 0.7503 - val_f2_score: 0.7664 - val_loss: 0.5486 - val_precision: 0.7304 - val_recall: 0.7760\n",
      "Epoch 48/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7539 - f2_score: 0.7904 - loss: 0.5439 - precision: 0.7246 - recall: 0.8088 - val_accuracy: 0.7537 - val_f2_score: 0.8197 - val_loss: 0.5459 - val_precision: 0.7048 - val_recall: 0.8545\n",
      "Epoch 49/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7539 - f2_score: 0.7904 - loss: 0.5439 - precision: 0.7246 - recall: 0.8088 - val_accuracy: 0.7537 - val_f2_score: 0.8197 - val_loss: 0.5459 - val_precision: 0.7048 - val_recall: 0.8545\n",
      "Epoch 49/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7443 - f2_score: 0.7767 - loss: 0.5452 - precision: 0.7184 - recall: 0.7928 - val_accuracy: 0.7492 - val_f2_score: 0.8373 - val_loss: 0.5456 - val_precision: 0.6901 - val_recall: 0.8845\n",
      "Epoch 50/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7443 - f2_score: 0.7767 - loss: 0.5452 - precision: 0.7184 - recall: 0.7928 - val_accuracy: 0.7492 - val_f2_score: 0.8373 - val_loss: 0.5456 - val_precision: 0.6901 - val_recall: 0.8845\n",
      "Epoch 50/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7485 - f2_score: 0.7899 - loss: 0.5446 - precision: 0.7170 - recall: 0.8105 - val_accuracy: 0.7661 - val_f2_score: 0.8147 - val_loss: 0.5370 - val_precision: 0.7251 - val_recall: 0.8406\n",
      "Epoch 51/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7485 - f2_score: 0.7899 - loss: 0.5446 - precision: 0.7170 - recall: 0.8105 - val_accuracy: 0.7661 - val_f2_score: 0.8147 - val_loss: 0.5370 - val_precision: 0.7251 - val_recall: 0.8406\n",
      "Epoch 51/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7539 - f2_score: 0.7916 - loss: 0.5476 - precision: 0.7239 - recall: 0.8105 - val_accuracy: 0.7548 - val_f2_score: 0.8215 - val_loss: 0.5384 - val_precision: 0.7053 - val_recall: 0.8568\n",
      "Epoch 52/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7539 - f2_score: 0.7916 - loss: 0.5476 - precision: 0.7239 - recall: 0.8105 - val_accuracy: 0.7548 - val_f2_score: 0.8215 - val_loss: 0.5384 - val_precision: 0.7053 - val_recall: 0.8568\n",
      "Epoch 52/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7448 - f2_score: 0.7895 - loss: 0.5450 - precision: 0.7118 - recall: 0.8117 - val_accuracy: 0.7446 - val_f2_score: 0.7678 - val_loss: 0.5502 - val_precision: 0.7207 - val_recall: 0.7806\n",
      "Epoch 53/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7448 - f2_score: 0.7895 - loss: 0.5450 - precision: 0.7118 - recall: 0.8117 - val_accuracy: 0.7446 - val_f2_score: 0.7678 - val_loss: 0.5502 - val_precision: 0.7207 - val_recall: 0.7806\n",
      "Epoch 53/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7491 - f2_score: 0.7851 - loss: 0.5429 - precision: 0.7206 - recall: 0.8031 - val_accuracy: 0.7559 - val_f2_score: 0.8337 - val_loss: 0.5414 - val_precision: 0.7006 - val_recall: 0.8753\n",
      "Epoch 54/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7491 - f2_score: 0.7851 - loss: 0.5429 - precision: 0.7206 - recall: 0.8031 - val_accuracy: 0.7559 - val_f2_score: 0.8337 - val_loss: 0.5414 - val_precision: 0.7006 - val_recall: 0.8753\n",
      "Epoch 54/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7542 - f2_score: 0.7917 - loss: 0.5384 - precision: 0.7243 - recall: 0.8105 - val_accuracy: 0.7390 - val_f2_score: 0.8045 - val_loss: 0.5496 - val_precision: 0.6927 - val_recall: 0.8383\n",
      "Epoch 55/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7542 - f2_score: 0.7917 - loss: 0.5384 - precision: 0.7243 - recall: 0.8105 - val_accuracy: 0.7390 - val_f2_score: 0.8045 - val_loss: 0.5496 - val_precision: 0.6927 - val_recall: 0.8383\n",
      "Epoch 55/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7545 - f2_score: 0.7937 - loss: 0.5403 - precision: 0.7235 - recall: 0.8134 - val_accuracy: 0.7503 - val_f2_score: 0.7944 - val_loss: 0.5472 - val_precision: 0.7137 - val_recall: 0.8176\n",
      "Epoch 56/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7545 - f2_score: 0.7937 - loss: 0.5403 - precision: 0.7235 - recall: 0.8134 - val_accuracy: 0.7503 - val_f2_score: 0.7944 - val_loss: 0.5472 - val_precision: 0.7137 - val_recall: 0.8176\n",
      "Epoch 56/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7429 - f2_score: 0.7813 - loss: 0.5467 - precision: 0.7136 - recall: 0.8002 - val_accuracy: 0.7525 - val_f2_score: 0.8103 - val_loss: 0.5474 - val_precision: 0.7082 - val_recall: 0.8406\n",
      "Epoch 57/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7429 - f2_score: 0.7813 - loss: 0.5467 - precision: 0.7136 - recall: 0.8002 - val_accuracy: 0.7525 - val_f2_score: 0.8103 - val_loss: 0.5474 - val_precision: 0.7082 - val_recall: 0.8406\n",
      "Epoch 57/150\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7474 - f2_score: 0.7915 - loss: 0.5386 - precision: 0.7144 - recall: 0.8134 - val_accuracy: 0.7492 - val_f2_score: 0.8182 - val_loss: 0.5541 - val_precision: 0.6994 - val_recall: 0.8545\n",
      "Epoch 57: early stopping\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "\n",
      "âœ“ Training completed\n",
      "âœ“ Best validation loss: 0.5302\n",
      "âœ“ Best validation F2-Score: 0.8373\n",
      "âœ“ Final validation precision: 0.6994\n",
      "âœ“ Final validation recall: 0.8545\n",
      "\u001b[1m111/111\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7474 - f2_score: 0.7915 - loss: 0.5386 - precision: 0.7144 - recall: 0.8134 - val_accuracy: 0.7492 - val_f2_score: 0.8182 - val_loss: 0.5541 - val_precision: 0.6994 - val_recall: 0.8545\n",
      "Epoch 57: early stopping\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "\n",
      "âœ“ Training completed\n",
      "âœ“ Best validation loss: 0.5302\n",
      "âœ“ Best validation F2-Score: 0.8373\n",
      "âœ“ Final validation precision: 0.6994\n",
      "âœ“ Final validation recall: 0.8545\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAIN FINAL MODEL WITH BEST HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING FINAL MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Build the best model\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "print(\"\\nğŸ“Š Model Architecture (with best hyperparameters):\")\n",
    "model.summary()\n",
    "\n",
    "# Configure callbacks for final training\n",
    "early_stopping_final = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor binary crossentropy loss\n",
    "    mode='min',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the final model\n",
    "print(\"\\nğŸš€ Training...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping_final],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Training completed\")\n",
    "print(f\"âœ“ Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"âœ“ Best validation F2-Score: {max(history.history['val_f2_score']):.4f}\")\n",
    "print(f\"âœ“ Final validation precision: {history.history['val_precision'][-1]:.4f}\")\n",
    "print(f\"âœ“ Final validation recall: {history.history['val_recall'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f91bbef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATING ON TEST SET\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ˆ Test Metrics (threshold=0.5):\n",
      "ğŸ“ˆ Test Metrics (threshold=0.5):\n",
      "  â€¢ Loss (binary crossentropy): 0.5450\n",
      "  â€¢ Accuracy: 0.7425\n",
      "  â€¢ Precision: 0.7150\n",
      "  â€¢ Recall: 0.7949\n",
      "  â€¢ F2-Score: 0.7775\n",
      "\n",
      "  â€¢ Loss (binary crossentropy): 0.5450\n",
      "  â€¢ Accuracy: 0.7425\n",
      "  â€¢ Precision: 0.7150\n",
      "  â€¢ Recall: 0.7949\n",
      "  â€¢ F2-Score: 0.7775\n",
      "\n",
      "ğŸ” Pathological Solution Check:\n",
      "  â€¢ Positive prediction rate: 54.83%\n",
      "  â€¢ Test precision: 0.7150\n",
      "  âœ“ Model appears balanced (precision â‰¥ 0.30, positive rate â‰¤ 80%)\n",
      "\n",
      "ğŸ“Š Confusion Matrix:\n",
      "  TN=  388  FP=  173\n",
      "  FN=  112  TP=  434\n",
      "\n",
      "ğŸ” Pathological Solution Check:\n",
      "  â€¢ Positive prediction rate: 54.83%\n",
      "  â€¢ Test precision: 0.7150\n",
      "  âœ“ Model appears balanced (precision â‰¥ 0.30, positive rate â‰¤ 80%)\n",
      "\n",
      "ğŸ“Š Confusion Matrix:\n",
      "  TN=  388  FP=  173\n",
      "  FN=  112  TP=  434\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUATE MODEL ON TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate with automatic metrics (threshold=0.5 fixed)\n",
    "test_loss, test_acc, test_prec, test_rec, test_f2 = model.evaluate(\n",
    "    X_test_scaled, y_test,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“ˆ Test Metrics (threshold=0.5):\")\n",
    "print(f\"  â€¢ Loss (binary crossentropy): {test_loss:.4f}\")\n",
    "print(f\"  â€¢ Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  â€¢ Precision: {test_prec:.4f}\")\n",
    "print(f\"  â€¢ Recall: {test_rec:.4f}\")\n",
    "print(f\"  â€¢ F2-Score: {test_f2:.4f}\")\n",
    "\n",
    "# Generate predictions for additional validation\n",
    "y_pred_proba = model.predict(X_test_scaled, verbose=0).flatten()\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Check for pathological solutions\n",
    "positive_rate = y_pred.mean()\n",
    "print(\"\\nğŸ” Pathological Solution Check:\")\n",
    "print(f\"  â€¢ Positive prediction rate: {positive_rate:.2%}\")\n",
    "print(f\"  â€¢ Test precision: {test_prec:.4f}\")\n",
    "\n",
    "if positive_rate > 0.80:\n",
    "    print(\"  âš ï¸ WARNING: Model predicts positive >80% of the time\")\n",
    "if test_prec < 0.30:\n",
    "    print(\"  âš ï¸ WARNING: Precision is very low (<0.30)\")\n",
    "    \n",
    "if positive_rate <= 0.80 and test_prec >= 0.30:\n",
    "    print(\"  âœ“ Model appears balanced (precision â‰¥ 0.30, positive rate â‰¤ 80%)\")\n",
    "\n",
    "# Show confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nğŸ“Š Confusion Matrix:\")\n",
    "print(f\"  TN={cm[0,0]:5d}  FP={cm[0,1]:5d}\")\n",
    "print(f\"  FN={cm[1,0]:5d}  TP={cm[1,1]:5d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ed16cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAVING ARTIFACTS\n",
      "======================================================================\n",
      "\n",
      "âœ“ Model saved: Models\\model_v2.0.2.h5\n",
      "âœ“ Scaler saved: Models\\scaler_v2.0.2.pkl\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSAVING ARTIFACTS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m model_path, scaler_path, hps_path, metadata_path = \u001b[43msave_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbest_hps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_hps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2.0.2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     79\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36msave_artifacts\u001b[39m\u001b[34m(model, scaler, best_hps, version)\u001b[39m\n\u001b[32m     38\u001b[39m hps_dict = best_hps.values.copy()\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(hps_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[43mjson\u001b[49m.dump(hps_dict, f, indent=\u001b[32m2\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Hyperparameters saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhps_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Save metadata\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE ARTIFACTS (Model, Scaler, Hyperparameters, Metadata)\n",
    "# ============================================================================\n",
    "\n",
    "def save_artifacts(model, scaler, best_hps, version='2.0.2'):\n",
    "    \"\"\"\n",
    "    Save only essential artifacts for model deployment.\n",
    "    \n",
    "    Saves:\n",
    "    - model_v{version}.h5: Trained Keras model\n",
    "    - scaler_v{version}.pkl: StandardScaler for preprocessing\n",
    "    - hyperparameters_v{version}.json: Best hyperparameters found\n",
    "    - metadata_v{version}.txt: Human-readable summary\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import joblib\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create Models directory if it doesn't exist\n",
    "    models_dir = 'Models'\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # Define file paths with related names\n",
    "    model_path = os.path.join(models_dir, f'model_v{version}.h5')\n",
    "    scaler_path = os.path.join(models_dir, f'scaler_v{version}.pkl')\n",
    "    hps_path = os.path.join(models_dir, f'hyperparameters_v{version}.json')\n",
    "    metadata_path = os.path.join(models_dir, f'metadata_v{version}.txt')\n",
    "    \n",
    "    # Save model\n",
    "    model.save(model_path)\n",
    "    print(f\"\\nâœ“ Model saved: {model_path}\")\n",
    "    \n",
    "    # Save scaler\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"âœ“ Scaler saved: {scaler_path}\")\n",
    "    \n",
    "    # Save hyperparameters\n",
    "    hps_dict = best_hps.values.copy()\n",
    "    with open(hps_path, 'w') as f:\n",
    "        json.dump(hps_dict, f, indent=2)\n",
    "    print(f\"âœ“ Hyperparameters saved: {hps_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        f.write(f\"Model Version: {version}\\n\")\n",
    "        f.write(f\"Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Number of features: {len(FEATURES)}\\n\")\n",
    "        f.write(f\"\\nTest Performance (threshold=0.5):\\n\")\n",
    "        f.write(f\"  F2-Score: {test_f2:.4f}\\n\")\n",
    "        f.write(f\"  Precision: {test_prec:.4f}\\n\")\n",
    "        f.write(f\"  Recall: {test_rec:.4f}\\n\")\n",
    "        f.write(f\"  Accuracy: {test_acc:.4f}\\n\")\n",
    "        f.write(f\"\\nOptimization Strategy:\\n\")\n",
    "        f.write(f\"  Loss Function: binary_crossentropy\\n\")\n",
    "        f.write(f\"  Primary Objective: val_f2_score (maximize)\\n\")\n",
    "        f.write(f\"  Secondary Objective: val_precision (maximize)\\n\")\n",
    "        f.write(f\"  Threshold: 0.5 (fixed)\\n\")\n",
    "        f.write(f\"\\nHyperparameters:\\n\")\n",
    "        for key, value in hps_dict.items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "        f.write(f\"\\nFeatures ({len(FEATURES)}):\\n\")\n",
    "        for i, feat in enumerate(FEATURES, 1):\n",
    "            f.write(f\"  {i:2d}. {feat}\\n\")\n",
    "    print(f\"âœ“ Metadata saved: {metadata_path}\")\n",
    "    \n",
    "    print(f\"\\nâœ… All artifacts saved successfully to {models_dir}/\")\n",
    "    return model_path, scaler_path, hps_path, metadata_path\n",
    "\n",
    "# Save everything\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING ARTIFACTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_path, scaler_path, hps_path, metadata_path = save_artifacts(\n",
    "    model=model,\n",
    "    scaler=scaler,\n",
    "    best_hps=best_hps,\n",
    "    version='2.0.2'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f73d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PRODUCTION USAGE EXAMPLE\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model and scaler loaded successfully\n",
      "\n",
      "Example predictions:\n",
      "Sample   Probability     Prediction      Actual    \n",
      "--------------------------------------------------\n",
      "1        0.9947 ( 99.5%)  Flood           No Flood   âœ—\n",
      "2        0.9986 ( 99.9%)  Flood           No Flood   âœ—\n",
      "3        1.0000 (100.0%)  Flood           Flood      âœ“\n",
      "4        0.9962 ( 99.6%)  Flood           No Flood   âœ—\n",
      "5        1.0000 (100.0%)  Flood           Flood      âœ“\n",
      "\n",
      "âœ… Successfully demonstrated how to load and use the model\n",
      "\n",
      "Example predictions:\n",
      "Sample   Probability     Prediction      Actual    \n",
      "--------------------------------------------------\n",
      "1        0.9947 ( 99.5%)  Flood           No Flood   âœ—\n",
      "2        0.9986 ( 99.9%)  Flood           No Flood   âœ—\n",
      "3        1.0000 (100.0%)  Flood           Flood      âœ“\n",
      "4        0.9962 ( 99.6%)  Flood           No Flood   âœ—\n",
      "5        1.0000 (100.0%)  Flood           Flood      âœ“\n",
      "\n",
      "âœ… Successfully demonstrated how to load and use the model\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# USAGE EXAMPLE: How to load and use the saved model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"USAGE EXAMPLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load saved artifacts\n",
    "print(\"\\nğŸ“¦ Loading saved artifacts...\")\n",
    "loaded_model = keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects={'F2Score': F2Score}  # IMPORTANT: Register custom metric\n",
    ")\n",
    "loaded_scaler = joblib.load(scaler_path)\n",
    "\n",
    "with open(hps_path, 'r') as f:\n",
    "    loaded_hps = json.load(f)\n",
    "\n",
    "print(\"âœ“ Model loaded successfully\")\n",
    "print(\"âœ“ Scaler loaded successfully\")\n",
    "print(\"âœ“ Hyperparameters loaded successfully\")\n",
    "\n",
    "# Example prediction on test set (first 5 samples)\n",
    "print(\"\\nğŸ”® Example predictions (first 5 test samples):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "sample_X = X_test[:5]\n",
    "sample_y_true = y_test[:5]\n",
    "\n",
    "# Preprocess and predict\n",
    "sample_X_scaled = loaded_scaler.transform(sample_X)\n",
    "sample_y_pred_proba = loaded_model.predict(sample_X_scaled, verbose=0).flatten()\n",
    "sample_y_pred = (sample_y_pred_proba >= 0.5).astype(int)  # Fixed threshold 0.5\n",
    "\n",
    "# Display results\n",
    "for i in range(5):\n",
    "    true_label = \"Flood\" if sample_y_true[i] == 1 else \"No Flood\"\n",
    "    pred_label = \"Flood\" if sample_y_pred[i] == 1 else \"No Flood\"\n",
    "    prob = sample_y_pred_proba[i]\n",
    "    \n",
    "    match = \"âœ“\" if sample_y_true[i] == sample_y_pred[i] else \"âœ—\"\n",
    "    print(f\"Sample {i+1}: True={true_label:8s} | Pred={pred_label:8s} | Prob={prob:.3f} | {match}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… WORKFLOW COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Points:\")\n",
    "print(\"  â€¢ Loss function: binary_crossentropy (proper probabilistic training)\")\n",
    "print(\"  â€¢ F2-score: metric only (not loss)\")\n",
    "print(\"  â€¢ Optimization: dual objectives (F2-score + precision)\")\n",
    "print(\"  â€¢ Threshold: 0.5 (fixed for all metrics)\")\n",
    "print(\"  â€¢ Artifacts saved to: Models/\")\n",
    "print(\"\\nThe model is ready for deployment!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
